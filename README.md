In this repository, we develop a novel method for knowledge distillation between teacher-student vits. Our aim is to transfer knowledge from a large teacher model to a student model for ViTs. The plan that we have come up with is using the attention head representation from a teacher model to guide the training of a student model. We want to transfer the representation of the attention head of an already trained teacher model to enhance the corresponding representation in attention head of a student model
