We explore a novel method for knowledge distillation between teacher-student ViT. Our aim is to transfer knowledge from a large teacher model to a student model. We want to utilize the attention head representations of the teacher model to guide the training of a student model. As such, we transfer the representation of the attention head of an already trained teacher model to enhance the corresponding representation in attention head of a student model and see how that improves the student model's accuracy. 

