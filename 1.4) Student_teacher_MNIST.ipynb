{"cells":[{"cell_type":"markdown","metadata":{"id":"PAjivfuDBN7g"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28973,"status":"ok","timestamp":1713577388820,"user":{"displayName":"Rehan Ahmad","userId":"07009469059929470360"},"user_tz":-300},"id":"dGqpCGQpA6Nf","outputId":"16fcd3e8-fa78-47b8-fb81-8293c701b21b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import json\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib_inline.backend_inline\n","import seaborn as sns\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data as data\n","import torchvision\n","from torchvision import transforms\n","from torchvision.datasets import CIFAR10\n","import math\n","from prettytable import PrettyTable\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Z8xtm2S-FKE8"},"source":["# Our ViT modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jSoCjTK_Cfyv"},"outputs":[],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/ViT-codes')\n","from Modified_vit import ViTForClassification\n","from Modified_utils import visualize_attention, prepare_data,save_checkpoint,save_experiment,load_experiment, Modified_Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":450,"status":"error","timestamp":1713577354947,"user":{"displayName":"Rehan Ahmad","userId":"07009469059929470360"},"user_tz":-300},"id":"eTjRsj-gBmh3","outputId":"5c4bb0a0-78d0-4c39-a10d-d7486d3f9eba"},"outputs":[{"ename":"NameError","evalue":"name 'ViTForClassification' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-13f19577084a>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m }\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mViTForClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0msave_model_every_n_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'ViTForClassification' is not defined"]}],"source":["config = {\n","    \"patch_size\": 7,\n","    \"embed_dim\": 32,\n","    \"num_hidden_layers\": 4,\n","    \"num_attention_heads\": 3,\n","    \"hidden_dim\": 64,  # Adjusted as 2 * embed_dim\n","    \"dropout_val\": 0.1,\n","    \"attention_probs_dropout_prob\": 0.1,\n","    \"initializer_range\": 0.02,\n","    \"image_size\": 28,\n","    \"num_classes\": 10,\n","    \"num_channels\": 1,\n","    \"qkv_bias\": True,\n","    \"use_faster_attention\": True,\n","    \"attention_block_index\": 1  # Specify the block index you want to observe\n","}\n","\n","model = ViTForClassification(config)\n","\n","save_model_every_n_epochs = 5\n","exp_name = \"ViT_CIFAR100\"\n","batch_size = 64\n","epochs = 5\n","lr = 0.001\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","save_model_every_n_epochs = 5\n","\n","trainloader, testloader, _ = prepare_data(batch_size)\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","loss_fn = nn.CrossEntropyLoss()\n","trainer = Modified_Trainer(model, optimizer, loss_fn, \"ViT_Experiment\", device,config)\n","attention_matrices = trainer.train(trainloader, testloader, epochs,save_model_every_n_epochs, output_attentions=True)\n"]},{"cell_type":"markdown","metadata":{"id":"3dpv6K_yBURC"},"source":["# Examining Attention Matrices:\n","\n","Note that the input batch is of 64 and MNIST has a total of (4 x 4) patches in which we add + 1 CLS token. The dimension of each patch is (7 x 7) in our configuration. Then after flattening, we would have a dimension of 49 which we would embed in a dimension of 32 using E. Our input then is the following to the attention block: [64, 16, 32].The Query, Key would be of size [64, 16, 32] . The attention matrix should have a shape of [64, 17, 17], representing the attention score of each patch with respect to every other patch. Now, each attention head computes its own set of Key (K), Query (Q), and Value (V) matrices. The dimensions of each of these matrices per head are the same as described earlier: [batch size, number of patches, embedding dimension per head]. For n heads, we therefore obtain the following as attention matrix:\n","- [64, n, 17, 17]  \n","\n","where n is the number of attention-heads.\n","\n","Now, the first index of `attention-matrix` highlights the number of epochs. For example, if the epochs are number 5, then the index of `attention-matrix` would run from 0 to 4. The second index highlights the number of **tensors** collected in an epoch. Note that MNSIT contains 60000 images and our batch size is 64. If you divide 60000/64, you obtain 938 tensors. These are the exact number of values stored as you can verify if you print `attention_matrices[0][937]` you obtain a tensor but for `attention_matrices[0][938]` you get an index error.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1713569702295,"user":{"displayName":"Rehan Ahmad","userId":"07009469059929470360"},"user_tz":-300},"id":"fmLZSNg1BIPu","outputId":"b99772f7-53fe-4db8-d5cf-8baee0ee600d"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([64, 3, 17, 17])\n"]}],"source":["attention_matrix = attention_matrices[0][0]\n","print(attention_matrix[0].shape) # this is where the attention is [64,3,17,17]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1713569702295,"user":{"displayName":"Rehan Ahmad","userId":"07009469059929470360"},"user_tz":-300},"id":"qZC8CWlfFQX6","outputId":"d1a9e334-4e3d-4f41-b5f9-4ca2ea516602"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([32, 3, 17, 17])\n"]}],"source":["attention_matrix = attention_matrices[2][937]\n","print(attention_matrix[0].shape) # this is where the attention is [64,3,17,17]"]},{"cell_type":"markdown","metadata":{"id":"PNw5m5OmFUS4"},"source":["# Examining Attention matrix in a particular head\n","\n","Lets examine the attention matrix for epoch 3, batch 32, image 3, head 2:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1713569702295,"user":{"displayName":"Rehan Ahmad","userId":"07009469059929470360"},"user_tz":-300},"id":"nN9v_7J3FUp1","outputId":"323a61d7-5c25-4cf7-f782-6cddf8b93ea0"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([64, 3, 17, 17])\n"]}],"source":["attention_matrices_index = attention_matrices[4][55] # epoch 2 and batch 32 tensor\n","attention_matrix_at_indx = attention_matrices_index[0] # the actual index\n","print(attention_matrix_at_indx.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MGwZqVtvL2lH"},"outputs":[],"source":["attention_matrices_img = attention_matrix_at_indx[31] # for image 32"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1713569702296,"user":{"displayName":"Rehan Ahmad","userId":"07009469059929470360"},"user_tz":-300},"id":"BY92B_DBFXaZ","outputId":"07ee0d39-55aa-4dd4-c7cf-796e31ccaadc"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([17, 17])\n"]}],"source":["attention_matrix_head = attention_matrices_img[2] # attention matrix for head 2\n","print(attention_matrix_head.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1713569702296,"user":{"displayName":"Rehan Ahmad","userId":"07009469059929470360"},"user_tz":-300},"id":"7G6o1S4CFYzF","outputId":"4fd72a75-91b3-46d8-c4ca-1f82a7486cda"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.0144, 0.0152, 0.0117, 0.0095, 0.0059, 0.0249, 0.0443, 0.0182, 0.0896,\n","         0.0177, 0.4442, 0.1135, 0.0040, 0.0358, 0.0361, 0.0996, 0.0154],\n","        [0.0324, 0.0601, 0.0570, 0.0642, 0.0581, 0.0711, 0.0451, 0.0577, 0.0773,\n","         0.0566, 0.0710, 0.0633, 0.0458, 0.0418, 0.0574, 0.0815, 0.0597],\n","        [0.0590, 0.0577, 0.0563, 0.0610, 0.0711, 0.0623, 0.0491, 0.0773, 0.0640,\n","         0.0523, 0.0443, 0.0475, 0.0535, 0.0449, 0.0726, 0.0718, 0.0552],\n","        [0.1068, 0.0620, 0.0705, 0.0705, 0.1108, 0.0466, 0.0424, 0.0611, 0.0264,\n","         0.0580, 0.0094, 0.0187, 0.1290, 0.0492, 0.0564, 0.0227, 0.0597],\n","        [0.0360, 0.0361, 0.0303, 0.0350, 0.0305, 0.0511, 0.0604, 0.0678, 0.0867,\n","         0.0333, 0.1370, 0.0891, 0.0184, 0.0394, 0.0862, 0.1277, 0.0350],\n","        [0.0264, 0.0518, 0.0465, 0.0582, 0.0457, 0.0667, 0.0536, 0.0603, 0.0755,\n","         0.0476, 0.0970, 0.0817, 0.0350, 0.0388, 0.0641, 0.0985, 0.0525],\n","        [0.0657, 0.0745, 0.0932, 0.0765, 0.1357, 0.0507, 0.0218, 0.0385, 0.0238,\n","         0.0695, 0.0056, 0.0106, 0.1809, 0.0402, 0.0248, 0.0154, 0.0726],\n","        [0.0970, 0.0597, 0.0767, 0.0716, 0.0966, 0.0353, 0.0476, 0.0270, 0.0125,\n","         0.0657, 0.0049, 0.0125, 0.2209, 0.0653, 0.0355, 0.0075, 0.0635],\n","        [0.0145, 0.0379, 0.0321, 0.0426, 0.0239, 0.0565, 0.0512, 0.0426, 0.0834,\n","         0.0376, 0.1966, 0.1235, 0.0180, 0.0333, 0.0522, 0.1146, 0.0396],\n","        [0.0467, 0.0595, 0.0577, 0.0621, 0.0606, 0.0656, 0.0482, 0.0594, 0.0714,\n","         0.0586, 0.0603, 0.0593, 0.0506, 0.0476, 0.0641, 0.0701, 0.0582],\n","        [0.0196, 0.0870, 0.0975, 0.1011, 0.1076, 0.0766, 0.0208, 0.0304, 0.0389,\n","         0.0806, 0.0152, 0.0221, 0.1331, 0.0310, 0.0214, 0.0268, 0.0902],\n","        [0.0484, 0.0640, 0.0913, 0.0809, 0.1162, 0.0314, 0.0179, 0.0149, 0.0073,\n","         0.0700, 0.0017, 0.0058, 0.3292, 0.0371, 0.0120, 0.0033, 0.0685],\n","        [0.0474, 0.0442, 0.0391, 0.0397, 0.0391, 0.0559, 0.0595, 0.0621, 0.0873,\n","         0.0425, 0.1083, 0.0744, 0.0269, 0.0491, 0.0805, 0.1013, 0.0428],\n","        [0.0460, 0.0610, 0.0605, 0.0652, 0.0598, 0.0640, 0.0688, 0.0433, 0.0547,\n","         0.0623, 0.0499, 0.0516, 0.0671, 0.0615, 0.0722, 0.0485, 0.0637],\n","        [0.0552, 0.0402, 0.0388, 0.0382, 0.0278, 0.0426, 0.0807, 0.0500, 0.0597,\n","         0.0455, 0.1488, 0.1089, 0.0305, 0.0712, 0.0535, 0.0667, 0.0416],\n","        [0.0432, 0.0703, 0.0787, 0.0844, 0.0718, 0.0581, 0.0542, 0.0404, 0.0336,\n","         0.0734, 0.0319, 0.0452, 0.1149, 0.0603, 0.0349, 0.0290, 0.0758],\n","        [0.0353, 0.0615, 0.0589, 0.0664, 0.0630, 0.0709, 0.0423, 0.0617, 0.0751,\n","         0.0575, 0.0623, 0.0598, 0.0488, 0.0404, 0.0569, 0.0790, 0.0603]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)\n"]}],"source":["print(attention_matrix_head)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1713569702296,"user":{"displayName":"Rehan Ahmad","userId":"07009469059929470360"},"user_tz":-300},"id":"pKYYMutTFaBb","outputId":"0c621b21-0da6-4aba-b556-727265dd1398"},"outputs":[{"name":"stdout","output_type":"stream","text":["column sums are tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n","       device='cuda:0', grad_fn=<SumBackward1>)\n"]}],"source":["row_sums = torch.sum(attention_matrix_head, dim=-1)\n","print(\"column sums are\", row_sums) # AFTER REMOVING DROPOUT. WE ARE GETTING THE PROPER VALUES with each row summing to 1."]},{"cell_type":"markdown","metadata":{"id":"WIFAzeuUH7Rp"},"source":["# Implementing the Teacher-Student Trainer\n","\n","We setup a trainer where we first train a model for 100 epochs and then use it to distill knowledge to a student model."]},{"cell_type":"code","source":["class TS_Trainer:\n","    def __init__(self, teacher_model, student_model, optimizer_teacher, optimizer_student, loss_fn, exp_name, device, base_dir=\"experiments\"):\n","        self.teacher_model = teacher_model.to(device)\n","        self.student_model = student_model.to(device)\n","\n","        self.optimizer_teacher = optimizer_teacher\n","        self.optimizer_student = optimizer_student\n","        self.loss_fn = loss_fn\n","        self.exp_name = exp_name\n","        self.device = device\n","        self.base_dir = base_dir\n","\n","\n","    def train_epoch_teacher(self, trainloader):\n","        self.teacher_model.train()\n","        total_loss = 0\n","        for _, (images, labels) in enumerate(trainloader):\n","            images, labels = images.to(self.device), labels.to(self.device)\n","            self.optimizer_teacher.zero_grad()\n","            logits = self.teacher_model(images)[0]\n","            loss = self.loss_fn(logits, labels)\n","            loss.backward()\n","            self.optimizer_teacher.step()\n","            total_loss += loss.item() * len(images)\n","        return total_loss / len(trainloader.dataset)\n","\n","    def train_epoch_student(self,trainloader):\n","        self.student_model.train()\n","        total_loss = 0\n","        for _, (images, labels) in enumerate(trainloader):\n","          images, labels = images.to(self.device), labels.to(self.device)\n","          self.optimizer_student.zero_grad()\n","          # Forward passes\n","          student_logits, student_attention = self.student_model(images, output_attentions=True)\n","          with torch.no_grad(): # ENSURE THAT WE DON'T TRAIN THE TEACHER.\n","            _, teacher_attention = self.teacher_model(images, output_attentions=True)\n","\n","          # Compute losses\n","          classification_loss = self.loss_fn(student_logits, labels)\n","\n","          # Here we have to be careful. The actual student attention and\n","          # teacher attention are found in the first index [0].\n","          #They should be of dimensions [64 x 4 x 17 x 17]\n","          #print(student_attention[0].shape)\n","          #print(teacher_attention[0].shape)\n","\n","          student_attention_block = student_attention[0]\n","          teacher_attention_block = teacher_attention[0]\n","          attention_loss = nn.CrossEntropyLoss()(student_attention_block, teacher_attention_block)\n","          #print(\"attention loss is \",attention_loss)\n","          #print(\"classification loss is \",classification_loss )\n","          loss = classification_loss + attention_loss\n","\n","          # Backward and optimize\n","          loss.backward()\n","          self.optimizer_student.step()\n","          total_loss += loss.item() * images.size(0)\n","\n","        total_loss = total_loss / len(trainloader.dataset)\n","        classification_loss = classification_loss / len(trainloader.dataset)\n","        # Normalizing attention is reducing the value by too much.\n","        # However, that might just be because for now I am testing on a teacher model\n","        # that has not been trained properly.\n","\n","        #attention_loss = attention_loss / len(trainloader.dataset)\n","        print(\"attention loss for one batch \",attention_loss)\n","        print(\"classification loss for one batch \",classification_loss )\n","\n","        return total_loss, classification_loss, attention_loss\n","\n","\n","    def evaluate_teacher(self, testloader):\n","        self.teacher_model.eval()\n","        total_loss, correct = 0, 0\n","        for _, (images, labels) in enumerate(testloader):\n","            images, labels = images.to(self.device), labels.to(self.device)\n","            logits, _ = self.teacher_model(images)\n","            loss = self.loss_fn(logits, labels)\n","            total_loss += loss.item() * len(images)\n","            predictions = torch.argmax(logits, dim=1)\n","            correct += torch.sum(predictions == labels).item()\n","        accuracy = correct / len(testloader.dataset)\n","        avg_loss = total_loss / len(testloader.dataset)\n","        return accuracy, avg_loss\n","\n","    def evaluate_student(self, testloader):\n","        self.student_model.eval()\n","        total_loss, correct = 0, 0\n","        for _, (images, labels) in enumerate(testloader):\n","            images, labels = images.to(self.device), labels.to(self.device)\n","            logits, _ = self.student_model(images)\n","            # WE COMPUTE ONLY CLASSIFICATION LOSS WHEN EVALUATING STUDENT.\n","            # NO DISTILLATION LOSS COMPUTED HERE.\n","            loss = self.loss_fn(logits, labels)\n","            total_loss += loss.item() * len(images)\n","            predictions = torch.argmax(logits, dim=1)\n","            correct += torch.sum(predictions == labels).item()\n","        accuracy = correct / len(testloader.dataset)\n","        avg_loss = total_loss / len(testloader.dataset)\n","        return accuracy, avg_loss\n","\n","\n","\n","\n","    def train_teacher(self, trainloader,testloader, epochs,save_model_every_n_epochs=0):\n","\n","        train_losses, test_losses, accuracies = [], [], []\n","        self.teacher_model.train()\n","        for i in range(epochs):\n","          train_loss = self.train_epoch_teacher(trainloader)\n","          accuracy, test_loss = self.evaluate_teacher(testloader)\n","          train_losses.append(train_loss)\n","          test_losses.append(test_loss)\n","          accuracies.append(accuracy)\n","          print(f\"Epoch: {i+1}, Teacher Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n","\n","          # Save checkpoint if required\n","          if save_model_every_n_epochs > 0 and (i+1) % save_model_every_n_epochs == 0:\n","              save_checkpoint(self.exp_name, self.teacher_model, f\"epoch_{i+1}\", base_dir=self.base_dir)\n","              print(f'\\tSaving checkpoint at epoch {i+1}')\n","\n","        # Save the final model and experiment details at the end of training\n","        save_experiment(self.exp_name, config, self.teacher_model, train_losses, test_losses, accuracies, base_dir=self.base_dir)\n","        print(f'teacher_final and experiment details saved under {self.exp_name}')\n","\n","# THIS IS THE ACTUAL NOVELTY. WE EMPLEMENT THIS FROM SCRATCH.\n","    def train_student(self, trainloader,testloader, epoch_student,save_model_every_n_epochs):\n","        train_losses, test_losses, attention_losses, accuracies = [], [], [],[]\n","        self.student_model.train()\n","        for i in range(epochs):\n","          train_loss, classification_loss, attention_loss = self.train_epoch_student(trainloader)\n","          accuracy, test_loss = self.evaluate_student(testloader)\n","          train_losses.append(train_loss)\n","          test_losses.append(test_loss)\n","          attention_losses.append(attention_loss)\n","          accuracies.append(accuracy)\n","          print(f\"Epoch: {i+1}, Student Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f},Attention Loss: {attention_loss:.4f}, Accuracy: {accuracy:.4f}\")\n","\n","          # Save checkpoint if required\n","          if save_model_every_n_epochs > 0 and (i+1) % save_model_every_n_epochs == 0:\n","              save_checkpoint(self.exp_name, self.student_model, f\"epoch_{i+1}\", base_dir=self.base_dir)\n","              print(f'\\tSaving checkpoint at epoch {i+1}')\n","\n","        # Save the final model and experiment details at the end of training\n","        save_experiment(self.exp_name, config, self.student_model, train_losses, test_losses, accuracies, base_dir=self.base_dir)\n","        print(f'student_final and experiment details saved under {self.exp_name}')"],"metadata":{"id":"rHMlb58LyZZh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bB19koK4-TUw","executionInfo":{"status":"ok","timestamp":1713586188428,"user_tz":-300,"elapsed":924280,"user":{"displayName":"Rehan Ahmad","userId":"07009469059929470360"}},"outputId":"407b9798-3097-4a73-fc75-10f0e238382b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Train loss: 1.5082, Test loss: 0.9648, Accuracy: 0.6519\n","Epoch: 2, Train loss: 0.6869, Test loss: 0.3586, Accuracy: 0.8916\n","Epoch: 3, Train loss: 0.3789, Test loss: 0.2567, Accuracy: 0.9193\n","Epoch: 4, Train loss: 0.2987, Test loss: 0.1924, Accuracy: 0.9384\n","Epoch: 5, Train loss: 0.2583, Test loss: 0.1706, Accuracy: 0.9465\n","\tSaving checkpoint at epoch 5\n","Epoch: 6, Train loss: 0.2233, Test loss: 0.1483, Accuracy: 0.9523\n","Epoch: 7, Train loss: 0.2030, Test loss: 0.1357, Accuracy: 0.9542\n","Epoch: 8, Train loss: 0.1848, Test loss: 0.1246, Accuracy: 0.9603\n","Epoch: 9, Train loss: 0.1684, Test loss: 0.1074, Accuracy: 0.9643\n","Epoch: 10, Train loss: 0.1584, Test loss: 0.1018, Accuracy: 0.9654\n","\tSaving checkpoint at epoch 10\n","Epoch: 11, Train loss: 0.1474, Test loss: 0.1060, Accuracy: 0.9672\n","Epoch: 12, Train loss: 0.1373, Test loss: 0.1000, Accuracy: 0.9688\n","Epoch: 13, Train loss: 0.1317, Test loss: 0.1025, Accuracy: 0.9690\n","Epoch: 14, Train loss: 0.1258, Test loss: 0.0870, Accuracy: 0.9718\n","Epoch: 15, Train loss: 0.1232, Test loss: 0.0867, Accuracy: 0.9734\n","\tSaving checkpoint at epoch 15\n","Epoch: 16, Train loss: 0.1178, Test loss: 0.0844, Accuracy: 0.9727\n","Epoch: 17, Train loss: 0.1131, Test loss: 0.0746, Accuracy: 0.9763\n","Epoch: 18, Train loss: 0.1095, Test loss: 0.0795, Accuracy: 0.9737\n","Epoch: 19, Train loss: 0.1062, Test loss: 0.0758, Accuracy: 0.9765\n","Epoch: 20, Train loss: 0.1018, Test loss: 0.0707, Accuracy: 0.9766\n","\tSaving checkpoint at epoch 20\n","Final model and experiment details saved under ViT_Experiment\n"]}],"source":["# FIRST TRAINING A SMALLER MODEL NORMALLY:\n","\n","config = {\n","    \"patch_size\": 7,\n","    \"embed_dim\": 32,\n","    \"num_hidden_layers\": 2,\n","    \"num_attention_heads\": 4,\n","    \"hidden_dim\": 64,  # Adjusted as 2 * embed_dim\n","    \"dropout_val\": 0.1,\n","    \"attention_probs_dropout_prob\": 0.1,\n","    \"initializer_range\": 0.02,\n","    \"image_size\": 28,\n","    \"num_classes\": 10,\n","    \"num_channels\": 1,\n","    \"qkv_bias\": True,\n","    \"use_faster_attention\": True,\n","    \"attention_block_index\": 1  # Specify the block index you want to observe\n","}\n","\n","model = ViTForClassification(config)\n","\n","save_model_every_n_epochs = 5\n","exp_name = \"ViT_CIFAR100\"\n","batch_size = 64\n","epochs = 20\n","lr = 0.001\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","save_model_every_n_epochs = 5\n","\n","trainloader, testloader, _ = prepare_data(batch_size)\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","loss_fn = nn.CrossEntropyLoss()\n","trainer = Modified_Trainer(model, optimizer, loss_fn, \"ViT_Experiment\", device,config)\n","trainer.train(trainloader, testloader, epochs,save_model_every_n_epochs, output_attentions=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2637864,"status":"ok","timestamp":1713585245534,"user":{"displayName":"Rehan Ahmad","userId":"07009469059929470360"},"user_tz":-300},"id":"UHQRa5pVMEGt","outputId":"bb71d51b-bda5-45bb-a000-bce0d24b4efc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Teacher Train loss: 1.4118, Test loss: 0.8152, Accuracy: 0.7263\n","Epoch: 2, Teacher Train loss: 0.6444, Test loss: 0.3684, Accuracy: 0.8889\n","Epoch: 3, Teacher Train loss: 0.3665, Test loss: 0.2782, Accuracy: 0.9130\n","Epoch: 4, Teacher Train loss: 0.2769, Test loss: 0.1715, Accuracy: 0.9473\n","Epoch: 5, Teacher Train loss: 0.2311, Test loss: 0.1651, Accuracy: 0.9484\n","\tSaving checkpoint at epoch 5\n","Epoch: 6, Teacher Train loss: 0.2005, Test loss: 0.1458, Accuracy: 0.9565\n","Epoch: 7, Teacher Train loss: 0.1743, Test loss: 0.1176, Accuracy: 0.9632\n","Epoch: 8, Teacher Train loss: 0.1606, Test loss: 0.1223, Accuracy: 0.9644\n","Epoch: 9, Teacher Train loss: 0.1492, Test loss: 0.1033, Accuracy: 0.9665\n","Epoch: 10, Teacher Train loss: 0.1366, Test loss: 0.1070, Accuracy: 0.9663\n","\tSaving checkpoint at epoch 10\n","Epoch: 11, Teacher Train loss: 0.1318, Test loss: 0.1001, Accuracy: 0.9686\n","Epoch: 12, Teacher Train loss: 0.1209, Test loss: 0.0939, Accuracy: 0.9718\n","Epoch: 13, Teacher Train loss: 0.1156, Test loss: 0.0911, Accuracy: 0.9714\n","Epoch: 14, Teacher Train loss: 0.1085, Test loss: 0.0779, Accuracy: 0.9751\n","Epoch: 15, Teacher Train loss: 0.1034, Test loss: 0.0872, Accuracy: 0.9735\n","\tSaving checkpoint at epoch 15\n","Epoch: 16, Teacher Train loss: 0.0984, Test loss: 0.0805, Accuracy: 0.9747\n","Epoch: 17, Teacher Train loss: 0.0920, Test loss: 0.0712, Accuracy: 0.9789\n","Epoch: 18, Teacher Train loss: 0.0907, Test loss: 0.0731, Accuracy: 0.9778\n","Epoch: 19, Teacher Train loss: 0.0848, Test loss: 0.0682, Accuracy: 0.9793\n","Epoch: 20, Teacher Train loss: 0.0820, Test loss: 0.0665, Accuracy: 0.9796\n","\tSaving checkpoint at epoch 20\n","teacher_final and experiment details saved under ViT_Student_teacher\n","attention loss for one batch  tensor(0.3205, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(1.3392e-05, grad_fn=<DivBackward0>)\n","Epoch: 1, Student Train loss: 1.7231, Test loss: 0.7599,Attention Loss: 0.3205, Accuracy: 0.7573\n","attention loss for one batch  tensor(0.3174, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(5.8624e-06, grad_fn=<DivBackward0>)\n","Epoch: 2, Student Train loss: 0.9147, Test loss: 0.3413,Attention Loss: 0.3174, Accuracy: 0.8909\n","attention loss for one batch  tensor(0.3128, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(3.5529e-06, grad_fn=<DivBackward0>)\n","Epoch: 3, Student Train loss: 0.6935, Test loss: 0.2584,Attention Loss: 0.3128, Accuracy: 0.9190\n","attention loss for one batch  tensor(0.3080, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(4.2020e-06, grad_fn=<DivBackward0>)\n","Epoch: 4, Student Train loss: 0.6076, Test loss: 0.1983,Attention Loss: 0.3080, Accuracy: 0.9373\n","attention loss for one batch  tensor(0.3035, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(4.5527e-06, grad_fn=<DivBackward0>)\n","Epoch: 5, Student Train loss: 0.5555, Test loss: 0.1638,Attention Loss: 0.3035, Accuracy: 0.9493\n","\tSaving checkpoint at epoch 5\n","attention loss for one batch  tensor(0.3009, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(2.2440e-06, grad_fn=<DivBackward0>)\n","Epoch: 6, Student Train loss: 0.5212, Test loss: 0.1490,Attention Loss: 0.3009, Accuracy: 0.9535\n","attention loss for one batch  tensor(0.2997, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(1.8971e-06, grad_fn=<DivBackward0>)\n","Epoch: 7, Student Train loss: 0.4989, Test loss: 0.1541,Attention Loss: 0.2997, Accuracy: 0.9517\n","attention loss for one batch  tensor(0.2946, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(1.7610e-06, grad_fn=<DivBackward0>)\n","Epoch: 8, Student Train loss: 0.4830, Test loss: 0.1246,Attention Loss: 0.2946, Accuracy: 0.9604\n","attention loss for one batch  tensor(0.2996, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(5.5412e-07, grad_fn=<DivBackward0>)\n","Epoch: 9, Student Train loss: 0.4672, Test loss: 0.1321,Attention Loss: 0.2996, Accuracy: 0.9591\n","attention loss for one batch  tensor(0.2996, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(2.8492e-06, grad_fn=<DivBackward0>)\n","Epoch: 10, Student Train loss: 0.4577, Test loss: 0.1155,Attention Loss: 0.2996, Accuracy: 0.9639\n","\tSaving checkpoint at epoch 10\n","attention loss for one batch  tensor(0.2949, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(1.8588e-06, grad_fn=<DivBackward0>)\n","Epoch: 11, Student Train loss: 0.4477, Test loss: 0.1121,Attention Loss: 0.2949, Accuracy: 0.9658\n","attention loss for one batch  tensor(0.2988, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(4.5178e-06, grad_fn=<DivBackward0>)\n","Epoch: 12, Student Train loss: 0.4388, Test loss: 0.1009,Attention Loss: 0.2988, Accuracy: 0.9689\n","attention loss for one batch  tensor(0.2975, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(2.9837e-06, grad_fn=<DivBackward0>)\n","Epoch: 13, Student Train loss: 0.4309, Test loss: 0.1056,Attention Loss: 0.2975, Accuracy: 0.9667\n","attention loss for one batch  tensor(0.2941, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(2.5361e-06, grad_fn=<DivBackward0>)\n","Epoch: 14, Student Train loss: 0.4233, Test loss: 0.0886,Attention Loss: 0.2941, Accuracy: 0.9723\n","attention loss for one batch  tensor(0.2972, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(1.0497e-06, grad_fn=<DivBackward0>)\n","Epoch: 15, Student Train loss: 0.4219, Test loss: 0.0850,Attention Loss: 0.2972, Accuracy: 0.9730\n","\tSaving checkpoint at epoch 15\n","attention loss for one batch  tensor(0.2909, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(2.9285e-06, grad_fn=<DivBackward0>)\n","Epoch: 16, Student Train loss: 0.4124, Test loss: 0.0846,Attention Loss: 0.2909, Accuracy: 0.9733\n","attention loss for one batch  tensor(0.2931, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(2.7301e-06, grad_fn=<DivBackward0>)\n","Epoch: 17, Student Train loss: 0.4115, Test loss: 0.0782,Attention Loss: 0.2931, Accuracy: 0.9744\n","attention loss for one batch  tensor(0.2977, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(1.9115e-06, grad_fn=<DivBackward0>)\n","Epoch: 18, Student Train loss: 0.4074, Test loss: 0.0733,Attention Loss: 0.2977, Accuracy: 0.9764\n","attention loss for one batch  tensor(0.2940, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(8.7966e-07, grad_fn=<DivBackward0>)\n","Epoch: 19, Student Train loss: 0.4023, Test loss: 0.0896,Attention Loss: 0.2940, Accuracy: 0.9716\n","attention loss for one batch  tensor(0.2928, grad_fn=<DivBackward1>)\n","classification loss for one batch  tensor(1.1932e-06, grad_fn=<DivBackward0>)\n","Epoch: 20, Student Train loss: 0.3983, Test loss: 0.0775,Attention Loss: 0.2928, Accuracy: 0.9755\n","\tSaving checkpoint at epoch 20\n","student_final and experiment details saved under ViT_Student_teacher\n"]}],"source":["config_teacher = {\n","    \"patch_size\": 7,\n","    \"embed_dim\": 32,\n","    \"num_hidden_layers\": 4,\n","    \"num_attention_heads\": 4,\n","    \"hidden_dim\": 64,  # Adjusted as 2 * embed_dim\n","    \"dropout_val\": 0.1,\n","    \"attention_probs_dropout_prob\": 0.1,\n","    \"initializer_range\": 0.02,\n","    \"image_size\": 28,\n","    \"num_classes\": 10,\n","    \"num_channels\": 1,\n","    \"qkv_bias\": True,\n","    \"use_faster_attention\": True,\n","    \"attention_block_index\": 1  # Specify the block index you want to observe\n","}\n","\n","config_student = {\n","    \"patch_size\": 7,\n","    \"embed_dim\": 32,\n","    \"num_hidden_layers\": 2,\n","    \"num_attention_heads\": 4,\n","    \"hidden_dim\": 64,  # Adjusted as 2 * embed_dim\n","    \"dropout_val\": 0.1,\n","    \"attention_probs_dropout_prob\": 0.1,\n","    \"initializer_range\": 0.02,\n","    \"image_size\": 28,\n","    \"num_classes\": 10,\n","    \"num_channels\": 1,\n","    \"qkv_bias\": True,\n","    \"use_faster_attention\": True,\n","    \"attention_block_index\": 1  # We extract attention from first block\n","}\n","\n","\n","exp_name = \"ViT_Student_teacher\"\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","teacher_model = ViTForClassification(config_teacher)\n","student_model = ViTForClassification(config_student)\n","\n","optimizer_teacher = optim.Adam(teacher_model.parameters(), lr=0.001)\n","optimizer_student = optim.Adam(student_model.parameters(), lr=0.001)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","save_model_every_n_epochs = 5\n","epoch_teacher = 20  # Adjust the number of epochs as needed\n","epoch_student = 20  # Adjust the number of epochs as needed\n","batch_size = 64  # Adjust the batch size as needed\n","trainloader, testloader, _ = prepare_data(batch_size)\n","\n","\n","\n","trainer = TS_Trainer(teacher_model, student_model, optimizer_teacher, optimizer_student, loss_fn,exp_name, device,base_dir=\"experiments\")\n","trainer.train_teacher(trainloader,testloader, epoch_teacher,save_model_every_n_epochs)  # Train teacher for 100 epochs\n","trainer.train_student(trainloader,testloader, epoch_student,save_model_every_n_epochs)  # Train student for 100 epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VNO_SgbjORsG"},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyOZWj2iJrw11aGq8uA8y4bD"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}